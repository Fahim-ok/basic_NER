# -*- coding: utf-8 -*-
"""NER.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OhscUAaANjof_WN49MQvC0NTBqw6T4eE
"""

import re

def preprocess_text(text):
    # Text cleaning
    cleaned_text = re.sub(r"[^\w\s]", "", text)  # Remove punctuation
    cleaned_text = cleaned_text.lower()  # Convert to lowercase
    
    # Tokenization
    tokens = cleaned_text.split()  # Split into individual words
    
    return tokens

# Example usage
text = "John works at XYZ Corp. He lives in New York City."
tokens = preprocess_text(text)
print(tokens)

import spacy

def extract_features(text):
    nlp = spacy.load("en_core_web_sm")
    doc = nlp(text)
    
    features = []
    for token in doc:
        # Word-Level Features
        word = token.text
        pos = token.pos_
        
        # Contextual Features
        prev_word = doc[token.i - 1].text if token.i > 0 else None
        next_word = doc[token.i + 1].text if token.i < len(doc) - 1 else None
        
        # Word Shape Features
        is_capitalized = token.is_title
        
        # Build feature dictionary
        feature = {
            "word": word,
            "pos": pos,
            "prev_word": prev_word,
            "next_word": next_word,
            "is_capitalized": is_capitalized
        }
        
        features.append(feature)
    
    return features

# Example usage
text = "John works at XYZ Corp. He lives in New York City."
features = extract_features(text)
for feature in features:
    print(feature)

!pip install sklearn-crfsuite



import sklearn_crfsuite
from sklearn.preprocessing import LabelEncoder
from sklearn_crfsuite import metrics

# Prepare the training data
# Assuming you have a list of dictionaries containing features and corresponding entity labels
training_data = [
    {"word": "John", "pos": "PROPN", "prev_word": None, "next_word": "works", "is_capitalized": True, "label": "PERSON"},
    {"word": "works", "pos": "VERB", "prev_word": "John", "next_word": "at", "is_capitalized": False, "label": None},
    # Add more training instances
]

# Extract features and labels
X = [[data["word"], data["pos"], data["prev_word"], data["next_word"], data["is_capitalized"]] for data in training_data]
y = [data["label"] for data in training_data]

# Encode the labels
label_encoder = LabelEncoder()
encoded_labels = label_encoder.fit_transform(y)

# Split the data
# Split the features and encoded labels into training and testing sets
X_train = X  # Features
y_train = encoded_labels  # Encoded labels

# Feature Extraction
# No additional feature extraction needed for CRF

# Model Training
crf_model = sklearn_crfsuite.CRF()
crf_model.fit(X_train, y_train)

# Evaluate the model
# Assuming you have a separate testing set
testing_data = [
    {"word": "He", "pos": "PRON", "prev_word": "Corp.", "next_word": "lives", "is_capitalized": False},
    {"word": "lives", "pos": "VERB", "prev_word": "He", "next_word": "in", "is_capitalized": False},
    # Add more testing instances
]
X_test = [[data["word"], data["pos"], data["prev_word"], data["next_word"], data["is_capitalized"]] for data in testing_data]
y_test = [0, 0, ...]  # True labels

y_pred = crf_model.predict(X_test)
predicted_labels = label_encoder.inverse_transform(y_pred)

print(metrics.flat_classification_report(y_test, y_pred))


# abular-Specific Architectures:

#     TabNet
#     AutoInt
#     DeepFM
#     Wide & Deep

